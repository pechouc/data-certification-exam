{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import Sequential, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Sentiment Analysis of French Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDsXJRI3Wfrv"
   },
   "source": [
    "### Objectives\n",
    "1. Text cleaning\n",
    "2. Text preprocessing for custom embedding Neural Network\n",
    "3. Train RNN model for sentiment analysis\n",
    "\n",
    "‚ö†Ô∏è This notebook will be your final deliverable. \n",
    "- Make sure it can run \"restart and run all\"\n",
    "- Delete useless code cells\n",
    "- Do not \"clear output\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvwURl10Wmw1"
   },
   "source": [
    "# 0. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains 30,000 french reviews of movies, along with the binary class 1 (positive) or 0 (negative) score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>√áa commence √† devenir √©nervant d'avoir l'impre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>J'ai aim√© ce film, si il ressemble a un docume...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Une grosse merde ce haneke ce faire produire p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Beau m√©lodrame magnifiquement photographi√©, \"V...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A la poursuite du diamant vers est un film pro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29946</th>\n",
       "      <td>Le meilleur film de super-h√©ros derri√®re le ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29947</th>\n",
       "      <td>Un drame qui est d'une efficacit√© remarquable....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29948</th>\n",
       "      <td>Une daube hollywoodienne de plus, aucun int√©r√™...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29949</th>\n",
       "      <td>Et voil√† un nouveau biopic sur la star du X Li...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29950</th>\n",
       "      <td>Un film qui fait vieux, avec des acteurs pas t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29951 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review  polarity\n",
       "0      √áa commence √† devenir √©nervant d'avoir l'impre...         0\n",
       "1      J'ai aim√© ce film, si il ressemble a un docume...         1\n",
       "2      Une grosse merde ce haneke ce faire produire p...         0\n",
       "3      Beau m√©lodrame magnifiquement photographi√©, \"V...         1\n",
       "4      A la poursuite du diamant vers est un film pro...         1\n",
       "...                                                  ...       ...\n",
       "29946  Le meilleur film de super-h√©ros derri√®re le ba...         1\n",
       "29947  Un drame qui est d'une efficacit√© remarquable....         1\n",
       "29948  Une daube hollywoodienne de plus, aucun int√©r√™...         0\n",
       "29949  Et voil√† un nouveau biopic sur la star du X Li...         0\n",
       "29950  Un film qui fait vieux, avec des acteurs pas t...         0\n",
       "\n",
       "[29951 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the dataset for you\n",
    "data = pd.read_csv('https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 8472,
     "status": "ok",
     "timestamp": 1615382505157,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "IufC0UUhxyGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    15051\n",
      "0    14900\n",
      "Name: polarity, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We create features\n",
    "y = data.polarity\n",
    "X = data.review\n",
    "\n",
    "# We analyse class balance\n",
    "print(pd.value_counts(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 544,
     "status": "ok",
     "timestamp": 1615383356787,
     "user": {
      "displayName": "Bruno Lajoie",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg0dl6gThG8gOPbCvHbgt62zQnsi8cgbQ7C5HkD_Cg=s64",
      "userId": "15793030209206844069"
     },
     "user_tz": -60
    },
    "id": "yzIpNmSg0XV4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity: 0 \n",
      "\n",
      "√áa commence √† devenir √©nervant d'avoir l'impression de voir et revoir le m√™me genre de film √† savoir : la com√©die romantique, surement le genre le plus prolifique de le production fran√ßaise actuelle. Le probl√®me c'est que l'on a souvent affaire √† des niaiseries de faible niveau comme celui ci. Avec un scenario ultra balis√© et conventionnel, c'est √† se demander comment √ßa peut passer les portes d'un producteur. Bref cette sempiternel histoire d'un homme mentant au nom de l'amour pour reconqu√©rir une femme et qui √† la fin se prend son mensonge en pleine figure est d'une originalit√© affligeante, et ce n'est pas la pr√©sence au casting de l'ex miss m√©t√©o Charlotte Le Bon qui r√™ve surement d'avoir la m√™me carri√®re que Louise Bourgoin qui change la donne.\n"
     ]
    }
   ],
   "source": [
    "# We check various reviews\n",
    "print(f'polarity: {y[0]} \\n')\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Clean Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì We need to give a _quick & dirty_ cleaning to all the sentences in the dataset. Create a variable `X_clean` of similar shape, but with the following cleaning:\n",
    "- Replace french accents by their non-accentuated equivalent using the [unidecode.unidecode()](https://pypi.org/project/Unidecode/) method\n",
    "- Reduce all uppercases to lowercases\n",
    "- Remove any characters outside of a-z, for instance using `string.isalpha()`\n",
    "\n",
    "üòå You will be given the solution `X_clean` in the next question to make sure you can complete the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_out_non_letters(text):\n",
    "    new_text = ''\n",
    "    for character in text:\n",
    "        if character.isalpha() or character == ' ':\n",
    "            new_text += character\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 s, sys: 37 ms, total: 10.1 s\n",
      "Wall time: 10.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "X_clean = X.map(unidecode)\n",
    "X_clean = X_clean.map(lambda string: string.lower())\n",
    "X_clean = X_clean.map(filter_out_non_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ca commence a devenir enervant davoir limpression de voir et revoir le meme genre de film a savoir  la comedie romantique surement le genre le plus prolifique de le production francaise actuelle le probleme cest que lon a souvent affaire a des niaiseries de faible niveau comme celui ci avec un scenario ultra balise et conventionnel cest a se demander comment ca peut passer les portes dun producteur bref cette sempiternel histoire dun homme mentant au nom de lamour pour reconquerir une femme et qui a la fin se prend son mensonge en pleine figure est dune originalite affligeante et ce nest pas la presence au casting de lex miss meteo charlotte le bon qui reve surement davoir la meme carriere que louise bourgoin qui change la donne'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C14',\n",
    "    shape = X_clean.shape,\n",
    "    first_sentence = X_clean[0]\n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have clean sentences, we need to convert each one into a list of integers of fixed size\n",
    "- For example, the sentence: `\"this was good\"` should become something like `array([1, 3, 18, 0, 0, 0, ...0], dtype=int32)` where each integer match to a each _unique_ word in your corpus of sentences.\n",
    "\n",
    "‚ùì Create a numpy ndarray `X_input` of shape (29951, 100) that will be the direct input to your Neutral Network. \n",
    "\n",
    "- 29951 represents the number of reviews in the dataset `X_clean`\n",
    "- 100 represents the maximum number of words to keep for each movie review.\n",
    "- It must contain only numerical values, without any `NaN`\n",
    "- In the process, compute and save the number of _unique_ words in your cleaned corpus under `vocab_size` variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ First, you **must** start back from the clean solution below (14Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        ca commence a devenir enervant de voir et revo...\n",
       "1        aime ce film si il ressemble a un documentaire...\n",
       "2        une grosse merde ce haneke ce faire produire p...\n",
       "3        beau melodrame magnifiquement photographie ver...\n",
       "4        a la poursuite du diamant vers est un film pro...\n",
       "                               ...                        \n",
       "29946    le meilleur film de derriere le batman de nola...\n",
       "29947    un drame qui est efficacite remarquable un fil...\n",
       "29948    une daube hollywoodienne de plus aucun interet...\n",
       "29949    et voila un nouveau biopic sur la star du x li...\n",
       "29950    un film qui fait vieux avec des acteurs pas to...\n",
       "Name: review, Length: 29951, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean = pd.read_csv(\"https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_clean.csv\")['review']\n",
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "tokenizer.fit_on_texts(X_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(X_clean)\n",
    "\n",
    "sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29951, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   47,   458,     3,   739,  4022,     1,    51,     2,   390,\n",
       "           4,    37,   100,     1,     7,     3,   373,     5,    99,\n",
       "         655,   781,     4,   100,     4,    23,  8891,     1,     4,\n",
       "         631,   499,  2021,     4,   486,    15,     3,   179,  1204,\n",
       "           3,    13,  6898,     1,   950,   234,    41,   214,   981,\n",
       "          26,     6,    48,  1078,  8892,     2,  3230,     3,    28,\n",
       "        1015,   205,    47,    70,   223,     8,  2092,  1570,   121,\n",
       "          45, 15087,    71,   253, 23500,    27,   451,     1,    19,\n",
       "        9750,    12,   158,     2,    10,     3,     5,    64,    28,\n",
       "         228,    34,  3548,    11,   529,  1334,     9,   827,  2475,\n",
       "           2,    17,    14,     5,   378,    27,   165,     1,  3053,\n",
       "       12169], dtype=int32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input = np.array(sequences)\n",
    "print(X_input.shape)\n",
    "X_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(np.unique(X_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('C1415',\n",
    "    type_X = type(X_input),\n",
    "    shape = X_input.shape, \n",
    "    input_1 = X_input[1], \n",
    ")\n",
    "result.write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjM5UP5ZMbY_"
   },
   "source": [
    "# 3. Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùìCreate and fit a Neural Netork that takes `X_input` and `y` as input, to binary classify each sentence's sentiment\n",
    "\n",
    "- You cannot use transfer learning or other pre-existing Word2Vec models\n",
    "- You must use a \"recurrent\" architecture to _capture_ a notion of order in the sentences' words\n",
    "- The performance metrics for this task is \"accuracy\"\n",
    "- Store your model in a variable `model` \n",
    "- Store the result your `model.fit()` in a variable `history`. \n",
    "- ‚ö†Ô∏è `history.history` must comprises a measure of the `val_accuracy` at each epoch.\n",
    "- You don't need to cross-validate your model\n",
    "\n",
    "üòå Don't worry, you will not be judged on your computer power: You should be able to reach accuracy significantly better than baseline in less than 3 minutes even without GPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ But first, you **must** start back from the solution below (70Mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://wagon-public-datasets.s3.amazonaws.com/certification_paris_2021Q1/movies_X_input.csv'\n",
    "X_input = np.genfromtxt(url, delimiter=',', dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29951, 100)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(input_dim=100000, output_dim=10, input_length=100, mask_zero=True))\n",
    "    \n",
    "    model.add(layers.LSTM(10, activation='tanh', return_sequences=True))\n",
    "    \n",
    "    model.add(layers.LSTM(5, activation='tanh', return_sequences=False))\n",
    "    \n",
    "    model.add(layers.Dense(15, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 10)           1000000   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 100, 10)           840       \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 5)                 320       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 15)                90        \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 5)                 80        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 1,001,336\n",
      "Trainable params: 1,001,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "656/656 [==============================] - 107s 163ms/step - loss: 0.5874 - accuracy: 0.7327 - val_loss: 0.3827 - val_accuracy: 0.8512\n",
      "Epoch 2/1000\n",
      "656/656 [==============================] - 102s 156ms/step - loss: 0.2759 - accuracy: 0.8936 - val_loss: 0.2568 - val_accuracy: 0.8945\n",
      "Epoch 3/1000\n",
      "656/656 [==============================] - 104s 159ms/step - loss: 0.1942 - accuracy: 0.9258 - val_loss: 0.2373 - val_accuracy: 0.9013\n",
      "Epoch 4/1000\n",
      "656/656 [==============================] - 104s 158ms/step - loss: 0.1588 - accuracy: 0.9415 - val_loss: 0.2717 - val_accuracy: 0.8903\n",
      "Epoch 5/1000\n",
      "656/656 [==============================] - 103s 157ms/step - loss: 0.1398 - accuracy: 0.9469 - val_loss: 0.2440 - val_accuracy: 0.9045\n",
      "Epoch 6/1000\n",
      "656/656 [==============================] - 101s 154ms/step - loss: 0.1247 - accuracy: 0.9539 - val_loss: 0.3042 - val_accuracy: 0.8850\n",
      "Epoch 7/1000\n",
      "656/656 [==============================] - 101s 153ms/step - loss: 0.1139 - accuracy: 0.9587 - val_loss: 0.2887 - val_accuracy: 0.8998\n",
      "Epoch 8/1000\n",
      "656/656 [==============================] - 100s 153ms/step - loss: 0.1042 - accuracy: 0.9612 - val_loss: 0.2705 - val_accuracy: 0.9013\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_input, y,\n",
    "    validation_split=0.3, \n",
    "    epochs=1000, batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nbresult import ChallengeResult\n",
    "result = ChallengeResult('C1517',\n",
    "                         history=history.history)\n",
    "result.write()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNt966tqZXM2p288pQsUAUV",
   "name": "certification_DL_NLP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "211.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
